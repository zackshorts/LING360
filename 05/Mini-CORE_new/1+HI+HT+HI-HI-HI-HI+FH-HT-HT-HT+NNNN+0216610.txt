<0216610>
<http://www.cche.net/text/usersguides/therapy.asp>
<Rater 1: HI_FH *  * N * ID: AQR70X1KIOP6B>
<Rater 2: HI_HT *  * N * ID: A10GZ0SWBZIRBK>
<Rater 3: HI_HT *  * N * ID: AQHMXM5LLEP2N>
<Rater 4: HI_IS *  * N * ID: AGT0JMR9C58X3>

<h> Clinical Scenario
<p> You are working as an internal medicine resident in a rheumatology rotation and are seeing a 19 year-old woman who has had systemic lupus erythematosus diagnosed on the basis of a characteristic skin rash, arthritis, and renal disease. A renal biopsy has shown diffuse proliferative nephritis. A year ago, her creatinine was 140 micromoles/litre, six months ago 180, and in a blood sample taken a week before this clinic visit, 220. Over the last year she has been taking prednisone, and over the last six months, cyclophosphamide, both in appropriate doses.
<p> You are distressed by the rising creatinine and the rheumatology fellow with whom you discuss the problem suggests that you contact the hematology service to consider a trial of plasmapheresis. She states that plasmapheresis is effective in reducing the level of the antibodies responsible for the nephritis, and cites a number of trials that have suggested therapy is beneficial. When you ask her if any of the studies were randomized clinical trials, she acknowledges that she is uncertain.
<p> You present the dilemma to the attending physician who responds with a suggestion that, before you make a decision, you review the relevant literature. She recommends that you bring the patient back in two weeks, at which time you can offer her the appropriate therapy.
<h> The Search
<p> You decide that the most helpful article would include patients with severe lupus that threatens renal function and who are, already receiving immunosuppressive agents. Plasmapheresis must be compared with a control management strategy, and patients must be randomized to receive or not receive the plasmapheresis. Finally, the article must report clinically important outcomes, such as deterioration in renal function. You are familiar with the software program "Grateful Med" and use it for your search. The program provides a listing of Medical subject (MeSH) headings, and you quickly find that "lupus nephritis" is one such heading and "plasmapheresis" another. You add a methodological term that will restrict your results to high quality studies, "randomized controlled trial (PT)" (PT stands for publication type). The search, which you restrict to English-language articles, yields a total of three papers. One is a trial of prednisone and cyclophosphamide [1] ; a second examines the effect of plasmapheresis on risk of infection [2] . The third citation, which describes "a controlled trial of plasmapheresis", appears most likely to address the issue at hand, the effectiveness of plasmapheresis in improving clinically important outcomes.
<p> The relevant article is a randomized trial in which 46 patients received a standard therapeutic regimen of prednisone and cyclophosphamide, and 40 patients received standard therapy plus plasmapheresis [3] . Despite the fact that antibody levels decreased in those undergoing plasmapheresis, there was a trend toward a greater proportion of the plasmapheresis-treated patients dying (20% versus 13%) or developing renal failure (25% versus 17%). This seems to settle the issue of whether to offer your patient plasmapheresis. You wonder, however, whether the study could have led to an inaccurate or biased outcome. The remainder of this paper will provide you with the tools to address this question.
<h> Introduction
<p> In the first section in this series we introduced a framework for using the medical literature to solve patient problems and provide better clinical care [4] . This second section begins the discussion of how to use an article dealing with therapy or prevention. We will use the term therapy in a broad sense. As we've described elsewhere [5] , the same guides can be applied to evaluation of therapeutic interventions (directed at reducing symptoms and curing disease) and preventive interventions (directed at reducing the risk of disease, or disease complications).
<h> The framework
<p> As with articles on other clinical questions, one can usefully pose three questions about an article on therapy.
<p> Are the results of the study valid?
<p> This question has to do with the validity or accuracy of the results and considers whether the treatment effect reported in the article represents the true direction and magnitude of the treatment effect. Another way to state this question is: do these results represent an unbiased estimate of the treatment effect, or have they been influenced in some systematic fashion to lead to a false conclusion?
<p> What were the results?
<p> If the results are valid and the study likely yields an unbiased assessment of treatment effect, then the results are worth examining further. This second question considers the size and precision of the treatment's effect. The best estimate of that effect will be the study findings themselves; the precision of the estimate will be superior in larger studies.
<p> Will the results help me in caring for my patients?
<p> This question has two parts. First, are the results applicable to your patient? You should hesitate to institute the treatment if either your patient is too dissimlar from those in the trial, or if the outcome that has been improved isn't important to your patient. Second, if the results are applicable what is the net impact of the treatment? The impact depends both benefits and risks (side-effects and toxicity) of treatment and the consequences of withholding treatment. Thus, even an effective therapy might be withheld when a patient's prognosis is already good without treatment, especially when the treatment is accompanied by important side-effects and toxicity.
<p> We summarize our approach to evaluating and applying the results of articles addressing therapeutic effectiveness in Table 1 . Housestaff and practising physicians alike need an approach that is both efficient and comprehensive. We have therefore labelled validity criteria as "primary" -- those few that can quickly be applied by readers with limited time -- and "secondary" -- those which, though still important, can be reserved for articles that pass the initial guides and for readers who have both the need and the time for a deeper review.
<h> III. Will the results help me in caring for my patients?
<h> Are the results of this article valid?
<h> A. Primary Guides
<h> 1. Was the assignment of patients to treatment randomized?
<p> During the 1970s and early 1980s surgeons increasing undertook extracranial-intracranial bypass (that is, anastomosis of a branch of the external carotid artery, the superficial temporal, to a branch of the internal carotid artery, the middle cerebral). They believed it prevented strokes in patients whose symptomatic cerebrovascular disease was otherwise surgically inaccessible. This conviction was based on the comparison of clinical outcomes among non-randomized "cohorts" of patients who, for whatever reason, had and had not undergone this operation, for the former appeared to fare much better than the latter. To the surprise of many and the indignation of a few, a large multi-center randomized trial, in which patients were allocated to receive or forego this operation using a process analogous to flipping a coin, demonstrated that the only effect of surgery was to make patients worse off in the immediate post-surgical period; long-term outcome was unaffected [6] . Other surprises generated by randomized trials that contradicted the results of less rigorous trials include the demonstration that steroids may increase (rather than reduce) mortality in patients with sepsis [7] , that steroid injections do not ameliorate facet-joint back pain [8] , and that plasmapheresis does not benefit patients with polymyositis [9] . Such surprises may occur when treatments are assigned by random allocation, rather than by the conscious decisions of clinicians and patients. In short, clinical outcomes result from many causes, and treatment is just one of them: underlying severity of illness, the presence of comorbid conditions, and a host of other prognostic factors (unknown as well as known) often swamp any effect of therapy. Because these other features also influence the clinician's decision to offer the treatment at issue, nonrandomized studies of efficacy are inevitably limited in their ability to distinguish useful from useless or even harmful therapy. As confirmation of this fact, it turns out that studies in which treatment is allocated by any method other than randomization tend to show larger (and frequently "false-positive") treatment effects than do randomized trials [10] [11] [12] [13] . The beauty of randomization is that it assures, if sample size is sufficiently large, that both known and unknown determinants of outcome are evenly distributed between treatment and control groups.
<p> What can the clinician do if no one has done a randomized trial of the therapeutic question she faces? She still has to make a treatment decision, and so must rely on weaker studies. In a later article in this series devoted to deciding whether a therapy or an exposure causes harm (a situation when randomization is usually not possible) we deal with how to assess weaker study designs. For now, you should bear in mind that non-randomized studies provide much weaker evidence than do randomized trials.
<h> 2. Were all patients who entered the trial properly accounted for and attributed at its conclusion?
<p> This guide has two components: was followup complete? and, were patients analyzed in the groups to which they were randomized?
<h> a) Was followup complete?
<p> Every patient who entered the trial should be accounted for at its conclusion. If this is not done, or if substantial numbers of patients are reported as "lost to followup," the validity of the study is open to question. The greater the number of subjects who are lost, the more the trial may be subject to bias because patients who are lost often have different prognoses from those who are retained, and may disappear because they suffer adverse outcomes (even death) or because they are doing well (and so did not return to clinic to be assessed).
<p> Readers can decide for themselves when the loss to follow-up is excessive by assuming, in positive trials, that all patients lost from the treatment group did badly, and all lost from the control group did well, and then recalculating the outcomes under these assumptions. If the conclusions of the trial do not change, then the loss to follow-up was not excessive. If the conclusions would change, the strength of inference is weakened (that is, less confidence can be placed in the study results). The extent to which the inference is weakened will depend on how likely it is that treatment patients lost to followup all did badly, while control patients lost to followup all did well.
<h> b) Were patients analyzed in the groups to which they were randomized?
<p> As in routine practice, patients in randomized trials sometimes forget to take their medicine or even refuse their treatment altogether. Readers might, on first blush, agree that such patients who never actually received their assigned treatment should be excluded from analyses for efficacy. Not so.
<p> The reasons people don't take their medication are often related to prognosis. In a number of randomized trials non-compliant patients have fared worse than those who took their medication as instructed, even after taking into account all known prognostic factors, and even when their medications were placebos! [14] [15] [16] [17] [18] [19] Excluding non-compliant patients from the analysis leaves behind those who may be destined to have a better outcome and destroys the unbiased comparison provided by randomization.
<p> The situation is similar with surgical therapies. Some patients randomized to surgery never have the operation because they are too sick, or suffer the outcome of interest (such as stroke or myocardial infarction) before they get to the operating room. If investigators include such patients, who are destined to do badly, in the control arm but not in the surgical arm of a trial, even a useless surgical therapy will appear to be effective. However, the apparent effectiveness of surgery will come not from a benefit to those who have surgery, but the systematic exclusion of those with the poorest prognosis from the surgical group.
<p> This principle of attributing all patients to the group to which they were randomized results in an "intention-to-treat" analysis. This strategy preserves the value of randomization: prognostic factors that we know about, and those we don't know about, will be, on average, equally distributed in the two groups, and the effect we see will be just that due to the treatment assigned.
<h> B. Secondary Guides
<p> Patients who know that they are on a new, experimental treatment are likely to have an opinion about its efficacy, as are their clinicians or the other study personnel who are measuring responses to therapy. These opinions, whether optimistic or pessimistic, can systematically distort both the other aspects of treatment, and the reporting of treatment outcomes, thereby reducing our confidence in the study's results. In addition, unblinded study personnel who are measuring outcomes may provide different interpretations of marginal findings or differential encouragement during performance tests, either one of which can distort their results [20] .
<p> The best way of avoiding all this bias is double-blinding (sometimes referred to as double-masking), which is achieved in drug trials by administering a placebo, indistinguishable from active treatment in appearance, taste and texture but lacking the putative active ingredient, to the control group. When you read reports on treatments (such as trials of surgical therapies) in which patients and treating clinicians cannot be kept blind, you should note whether investigators have minimized bias by blinding those who assess clinical outcomes.
<h> 4. Were the groups similar at the start of the trial?
<p> For reassurance about a study's validity, readers would like to be informed that the treatment and control groups were similar for all the factors that determine the clinical outcomes of interest save one: whether they received the experimental therapy. Investigators provide this reassurance when they display the "entry" or "baseline" prognostic features of the treatment and control patients. Although we never will know whether similarity exists for the unknown prognostic factors, we are reassured when the known prognostic factors are nicely balanced. Randomization doesn't always produce groups balanced for known prognostic factors. When the groups are small, chance may place those with apparently better prognoses in one group. As sample size increases, this is less and less likely (this is analogous to multiple coin flips: one wouldn't be too surprised to see seven heads out of ten coin flips, but one would be very surprised to see seventy heads out of one hundred coin flips).
<p> The issue here is not whether there are statistically significant differences in known prognostic factors between treatment groups (in a randomized trial one knows in advance that any differences that did occur happened by chance) but rather the magnitude of these differences. If they are large, the validity of the study may be compromised. The stronger the relationship between the prognostic factors and outcome, and the smaller the trial, the more the differences between groups will weaken the strength of any inference about efficacy.
<p> All is not lost if the treatment groups are not similar at baseline. Statistical techniques permit adjustment of the study result for baseline differences. Accordingly, readers should look for documentation of similarity for relevant baseline characteristics, and if substantial differences exist should note whether the investigators conducted an analysis that adjusted for those differences. When both unadjusted and adjusted analyses reach the same conclusion, readers justifiably gain confidence in the validity of the study result.
<h> 5. Aside from the experimental intervention, were the groups treated equally?
<p> Care for experimental and control groups can differ in a number of ways besides the test therapy, and differences in care other than that under study can weaken distort the results. If one group received closer followup, events might be more likely to be reported, and patients may be treated more intensively with non-study therapies. For example, in trials of new forms of therapy for resistant rheumatoid arthritis, ancillary treatment with systemic steroids (extremely effective for relieving symptoms), if administered more frequently to the control group than to the treatment group, could obscure an experimental drug's true treatment effect (unless exacerbation requiring steroids were itself counted as an outcome).
<p> Interventions other than the treatment under study, when differentially applied to the treatment and control groups, often are called "cointerventions". Cointervention is a more serious problem when double-blinding is absent, or when the use of very effective non-study treatments is permitted at the physicians' discretion. Clinicians gain greatest confidence in the results when permissible cointerventions are described in the methods section and documented to be infrequent occurrences in the results.
<p> The foregoing five guides (two primary and three secondary), applied in sequence, will help the reader determine whether the results of an article on therapy are likely to be valid. If the results are valid, then the reader can proceed to consider the magnitude of the effect and the applicability to her patients.
<p> Scenario Resolution
<p> Readers may be interested in how well the trial of plasmapheresis in patients with lupus nephritis met the tests of validity. With respect to primary criteria, randomization was rigorously conducted, as treatment was assigned through a phone call to the study's Methods Center. One patient assigned to standard therapy was lost to followup, and all the other patients were analyzed in the group to which they had been assigned. With respect to secondary criteria, the study was not blinded, the two groups were comparable at the start of the trial, and the authors provide little information about comparability of other treatments.
<p> In the introductory paper in this series, we described the concept of strength of inference. The final assessment of validity is never a "yes" or "no" decision and must, to some extent, be subjective. We judge that the methods in this trial were, overall, strong, and provide a valid start for deciding whether or not to administer plasmapheresis to our patient with severe lupus nephritis.
<h> II. What were the results?
<p> Clinical Scenario
<p> You are a general internist who is asked to see a 65 year-old man with controlled hypertension and a six-month history of atrial fibrillation resistant to cardioversion. Although he has no evidence for valvular or coronary heart disease, the family physician who referred him to you wants your advice on whether the benefits of long-term anticoagulants (to reduce the risk of embolic stroke) outweigh their risks (of hemorrhage from anticoagulant therapy). The patient shares these concerns, and doesn't want to receive a treatment that would do more harm than good. You know that there have been randomized trials of warfarin for non-valvular atrial fibrillation, and decide that you'd better review one of them.
<p> The Search
<p> The ideal article addressing this clinical problem would include patients with non-valvular atrial fibrillation, and would compare the effect of warfarin and a control treatment, ideally a placebo, on the risk of emboli (including embolic stroke) and also on the risk of the complications of anticoagulation. Randomized, double-blind studies would provide the strongest evidence.
<p> In the software program "Grateful Med" you select a Medical subject (MeSH) heading that identifies your population, "atrial fibrillation," another that specifies the intervention, "warfarin", and a third that specifies the outcome of interest, "stroke" (which the software automatically converts to "explode cerebrovascular disorders" meaning that all articles indexed under cerebrovascular disorders or its subheadings are potential targets of the search), while restricting the search to English-language studies. To ensure that, at least on your first pass, you identify only the highest quality studies, you include the methodological term "randomized controlled trial (PT)" (PT stands for publication type). The search yields nine articles. Three are editorials or commentaries, one addresses prognosis, and one focuses on quality of life on anticoagulants. You decide to read the most recent of the four randomized trials [21] .
<p> Reading the study, you find it meets the validity criteria you learned about in a prior article in this series [22] . To answer your patient's and the referring physician's concerns, however, you need to delve further into the relation between benefits and risks.
<h> 1. How large was the treatment effect?
<p> Most frequently, randomized clinical trials carefully monitor how often patients experience some adverse event or outcome. Examples of these "dichotomous" outcomes ("yes" or "no" outcomes that either happen or don't happen) include cancer recurrence, myocardial infarction, and death. Patients either do or do not suffer an event, and the article reports the proportion of patients who develop such events. Consider, for example, a study in which 20% (0.20) of a control group died, but only 15% (0.15) of those receiving a new treatment died. How might these results be expressed? Table 2 provides a summary of ways of presenting the effects of therapy.
<h> Table 2: Measures of the effects of therapy
<p> Risk without therapy (Baseline risk):X
<p> 20/100=0.20 or 20%
<p> Risk with therapy: Y
<p> 15/100=0.15 or 15%
<p> Absolute Risk Reduction (Risk Difference): X - Y
<p> 0.20-0.15=0.05
<p> Relative Risk: Y/X
<p> 0.15/0.20 = 0.75
<p> Relative Risk Reduction (RRR): [1-Y/X] x 100 or [(X-Y) / X] x 100
<p> [1-0.75]x100=25% [0.05/0.20]x100=25%
<p> 95% Confidence Interval for the RRR
<p> -38% to +59%
<p> One way would be as the absolute difference (known as the absolute risk reduction, or risk difference), between the proportion who died in the control group (X) and the proportion who died in the treatment group (Y), or X - Y = 0.20 - 0.15 = 0.05. Another way to express the impact of treatment would be as a relative risk: the risk of events among patients on the new treatment, relative to that among controls, or Y/X = 0.15 / 0.20 = 0.75.
<p> The most commonly reported measure of dichotomous treatment effects is the complement of this relative risk, and is called the relative risk reduction (RRR). It is expressed as a per cent: (1 - Y/X) x 100% = (1 - 0.75) x 100% = 25%. A RRR of 25% means that the new treatment reduced the risk of death by 25% relative to that occurring among control patients; the greater the relative risk reduction, the more effective the therapy.
<h> 2. How precise was the estimate of treatment effect?
<p> The true risk reduction can never be known; all we have is the estimate provided by rigorous controlled trials, and the best estimate of the true treatment effect is that observed in the trial. This estimate is called a "point estimate" in order to remind us that, although the true value lies somewhere in its neighbourhood, it is unlikely to be precisely correct. Investigators tell us the neighbourhood within which the true effect likely lies by the statistical strategy of calculating confidence intervals [23] .
<p> We usually (though arbitrarily) use the 95% confidence interval which can be simply interpreted as defining the range that includes the true relative risk reduction 95% of the time. You'll seldom find the true RRR toward the extremes of this interval, and you'll find the true RRR beyond these extremes only 5% of the time, a property of the confidence interval that relates closely to the conventional level of "statistical significance" of p &lt; 0.05. We illustrate the use of confidence intervals in the following examples.
<p> If a trial randomized 100 patients each to treatment and control groups, and there were 20 deaths in the control group and 15 deaths in the treatment group, the authors would calculate a point estimate for the relative risk reduction of 25% (X = 20/100 or 0.20, Y = 15/100 or 0.15, and 1-Y/X = [1-0.75]x100 = 25%). You might guess, however, that the true relative risk reduction might be much smaller or much greater than this 25%, based on a difference of just 5 deaths. In fact, you surmise that the treatment might provide no benefit (an RRR of 0%) or even harm (a negative RRR). And you would be right -- in fact, these results are consistent with both an RRR of - 38% (that is, patients given the new treatment might be 38% more likely to die than control patients), and an RRR of nearly 59% (that is, patients subsequently receiving the new treatment might have a risk of dying less than 60% that of the risk in those who are not treated). In other words, the 95% confidence interval on this RRR is -38% to 59%, and the trial really hasn't helped us decide whether to offer the new treatment. What sort of study would be more helpful?
<p> What if the trial enrolled, not 100 patients per group, but 1,000 patients per group, and observed the same event rates as before, so that there were 200 deaths in the control group (X = 200/1,000 = 0.20) and 150 deaths in the treatment group (Y = 150/1,000 = 0.15). Again, the point estimate of the RRR is 25% (1-Y/X = 1 - [0.15/0.20] x 100 = 25%). In this larger trial, you might think that the true reduction in risk is much closer to 25% and, again, you would be right; the 95% confidence interval on the RRR for this set of results is all on the positive side of zero and runs from 9% to 41%.
<p> What these examples show is that the larger the sample size of a trial, the larger the number of outcome events, and the greater our confidence that the true relative risk reduction (or any other measure of efficacy) is close to what we have observed. In the second example above, the lowest plausible value for the RRR was 9% and the highest value 41%. The point estimate -- in this case 25% -- is the one value most likely to represent the true relative risk reduction. As one considers values farther and farther from the point estimate, they become less and less consistent with the observed relative risk reduction. By the time one crosses the upper or lower boundaries of the 95% confidence interval, the values are extremely unlikely to represent the true RRR, given the point estimate (that is, the observed RRR).
<p> Figure 1 represents the confidence intervals around the point estimate of a relative risk reduction of 25% in these two examples, with a risk reduction of 0 representing no treatment effect. In both scenarios the point estimate of the RRR is 25%, but the confidence interval is far narrower in the second scenario.
<h> Figure 1: Confidence Intervals Around Relative Risk
<p> It is evident that the larger the sample size, the narrower the confidence interval. When is the sample size big enough [24] ? In a "positive" study -- a study in which the authors conclude that the treatment is effective -- one can look at the lower boundary of the confidence interval. In the second example, this lower boundary was +9%. If this risk reduction (the lowest that is consistent with the study results) is still important, or "clinically significant," (that is, it is large enough for you to want to offer it to your patient), then the investigators have enrolled sufficient patients. If, on the other hand, you do not consider a RRR of 9% clinically significant, then the study cannot be considered definitive, even if its results are statistically significant (that is, they exclude a risk reduction of 0). Keep in mind that the probability of the true value being less that the lower boundary of the confidence interval is only 2.5%, and that a different criterion for the confidence interval (a 90% confidence interval, for instance) might be as or more appropriate. The confidence interval also helps us interpret "negative" studies in which the authors have concluded that the experimental treatment is no better than control therapy. All we need do is look at the upper boundary of the confidence interval. If the RRR at this upper boundary would, if true, be clinically important, the study has failed to exclude an important treatment effect. In the first example we presented in this section, the upper boundary of the confidence interval was a RRR of 59%. Clearly, if this represented the truth, the benefit of the treatment would be substantial, and we would conclude that, although the investigators had failed to prove that experimental treatment was better than placebo, they also had failed to prove that it was not; they could not exclude a large, positive treatment effect. Once again the clinician must bear in mind the privoso about the arbitrariness of the choice of 95% boundaries for the confidence interval. A reasonable alternative, a 90% confidence interval, would be somewhat narrower.
<p> What can the clinician do if the confidence interval around the RRR is not reported in the article? There are three approaches, and we present them in order of increasing complexity. The easiest approach is to examine the p-value. If the p-value is exactly 0.05, then the lower bound of the 95% confidence limit for the RRR has to lie exactly at zero (a relative risk of 1), and you cannot exclude the possibility that the treatment has no effect. As the p-value decreases below 0.05, the lower bound of the 95% confidence limit for the RRR rises above zero.
<p> A second approach, involving some quick mental arithmetic or a pencil and paper, can be used when the article includes the value for the standard error of the RRR (or of the relative risk). This is because the upper and lower boundaries of the 95% confidence interval for a RRR are the point estimate plus and minus twice this standard error.
<p> The third approach involves calculating the confidence intervals yourself [5] or asking the help of someone else (a statistician, for instance) to do so. Once you obtain the confidence intervals, you know how high and low the RRR might be (that is, you know the precision of the estimate of the treatment effect), and can interpret the results as described above.
<p> Not all randomized trials have dichotomous outcomes, nor should they. For example, a new treatment for patients with chronic lung disease may focus on increasing their exercise capacity. Thus, in a study of respiratory muscle training for patients with chronic airflow limitation, one primary outcome measured how far patients could walk in six minutes in an enclosed corridor [25] . This six-minute walk improved from an average of 406 to 416 meters (up 10 meters) in the experimental group receiving respiratory muscle training, and from 409 to 429 (up 20 meters) in the control group. The point estimate for improvement in the six-minute walk due to respiratory muscle training therefore was negative, at -10 meters (or a 10 meter difference in favour of the control group).
<p> Here too, you should look for the 95% confidence intervals around this difference in changes in exercise capacity and consider their implications. The investigators tell us that the lower boundary of the 95% confidence interval was - 26 (that is, the results are consistent with a difference of 26 meters in favour of the control treatment) and the upper boundary was +5 meters. Even in the best of circumstances, adding 5 meters to the 400 recorded at the start of the trial would not be important to the patient, and this result effectively excludes a clinically significant benefit of respiratory muscle training as applied in this study.
<p> Having determined the magnitude and precision of the treatment effect, readers now can turn to the final question of how to apply the article's results to their patients and clinical practice.
<h> Will the results help me in caring for my patients?
<h> 1. Can the results be applied to my patient care?
<p> The first issue to address is how confident you are that you can apply the results to a particular patient or patients in your practice. If the patient would have been enroled in the study had she been there -- that is, she meets all the inclusion criteria, and doesn't violate any of the exclusion criteria -- there is little question that the results are applicable. If this is not the case, and she would not have been eligible for the study, judgement is required. The study result probably applies even if, for example, she was 2 years too old for the study, had more severe disease, had previously been treated with a competing therapy, or had a comorbid condition. A better approach than rigidly applying the study's inclusion and exclusion criteria is to ask whether there is some compelling reason why the results should not be applied to the patient. A compelling reason usually won't be found, and most often you can generalize the results to your patient with confidence.
<p> A final issue arises when our patient fits the features of a subgroup of pateints in the trial report. In articles reporting the results of a trial (especially when the treatment doesn't appear to be efficacious for the average patient), the authors may have examined a large number of subgroups of patients at different stages of their illness, with different comorbid conditions, with different ages at entry, and the like. Quite often these subgroup analyses were not planned ahead of time, and the data are simply "dredged" to see what might turn up. Investigators may sometimes over-interpret these "data-dependent" analyses as demonstrating that the treatment really has a different effect in a subgroup of patients -- those who are older or sicker, for instance, may be held up as benefitting substantially more or less than other subgroups of patients in the trial. You can find guides for deciding whether to believe these subgroup analyses [26] , and we'll summarize them for you here: the treatment is really likely to benefit the subgroup more or less than the other patients if the difference in the effects of treatment in the subgroups is large; is very unlikely to occur by chance; results from an analysis specified as a hypothesis before the study began; was one of only a very few subgroup analyses that were carried out; and is replicated in other studies. To the extent that the subgroup analysis fails these criteria, clinicians should be increasingly sceptical about applying them to their patients.
<h> 2. Were all clinically important outcomes considered?
<p> Treatments are indicated when they provide important benefits. Demonstrating that a bronchodilator produces small increments in forced expired volume in patients with chronic airflow limitation, that a vasodilator improves cardiac output in heart failure patients, or that a lipid-lowering agent improves lipid profiles, does not necessarily provide a sufficient reason for administering these drugs. What is required is evidence that the treatments improve outcomes that are important to patients, such as reducing shortness of breath during the activities required for daily living, avoiding hospitalization for heart failure, or decreasing the risk of myocardial infarction. We can consider forced expired volume in one second, cardiac output, and the lipid profile "substitute endpoints". That is, the authors have substituted these physiologic measures for the important outcomes (shortness of breath, hospitalization, or myocardial infarction), usually because to confirm benefit on the latter they would have had to enroll many more patients and followed them for far longer periods of time.
<p> A dramatic recent example of the danger of substitute endpoints was found in the evaluation of the usefulness of antiarrhythmic drugs following myocardial infarction. Because such drugs had been shown to reduce abnormal ventricular depolarizations (the substitute endpoints) in the short run, it made sense that they should reduce the occurrence of life-threatening arrhythmias in the long run. A group of investigators performed randomized trials on three agents (encainide, flecainide and moricizine) previously shown to be effective in suppressing the substitute endpoint of abnormal ventricular depolarizations in order to determine whether they reduced mortality in patients with asymptomatic or mildly symptomatic arrhythmias following myocardial infarction. The investigators had to stop the trials when they discovered that mortality was substantially higher in patients receiving antiarrhythmic treatment than in those receiving placebo [27] [28] . Clinicians relying on the substitute endpoint of arrhythmia suppression would have continued to administer the three drugs to the considerable detriment of their patients.
<p> Even when investigators report favourable effects ot treatment on one clinically important outcome, clinicians must take care that there are no deleterious effects on other outcomes. For instance, as this series was in preparation the controversy continued over whether reducing lipids unexpectedly increases non-cardiovascular causes of death [29] . Cancer chemotherapy may lengthen life but decrease its quality. Finally, surgical trials often document prolonged life for those who survive the operation (yielding higher 3-year survival in those receiving surgery), but an immediate risk of dying during or shortly after surgery. Accordingly, users of the reports of surgical trials should look for information on immediate and early mortality (typically higher in the surgical group) in addition to longer-term results.
<p> If the article's results are generalizable to your patient and its outcomes are important, the next question concerns whether the probable treatment benefits are worth the effort that you and your patient must put into the enterprise. A 25% reduction in the risk of death may sound quite impressive, but its impact on your patient and practice may nevertheless be minimal. This notion is illustrated using a concept called "number needed to treat" (NNT) [30] .
<p> The impact of a treatment is related not only to its relative risk reduction, but also to the risk of the adverse outcome it is designed to prevent. Beta blockers reduce the risk of death following myocardial infarction by approximately 25%, and this RRR is consistent across subgroups, including those at higher and lower "baseline" risk of recurrence and death when they are untreated. Table 3 considers two patients with recent myocardial infarctions.
<h> Table 3: Two men with contrasting prognoses following myocardial infarction
<p> If the risk of death at one year without therapy (baseline risk) is: X
<p> First, consider a 40 year old man with a small infarct, normal exercise capacity, and no sign of ventricular arrhythmia who is willing to stop smoking, begin exercising, lose weight, and take aspirin daily. This individual's risk of death in the first year after infarction may be as low as 1%. Beta blockers would reduce this risk by a quarter, to 0.75%, for an absolute risk reduction of 0.25% or 0.0025. The inverse of this absolute risk reduction (that is, 1 / absolute risk reduction) equals the number of such patients we'd have to treat in order to prevent one event (in this case, to prevent one death following a mild heart attack in a low risk patient). In this case, we would have to treat 400 such patients for one year to save a single life (1 / 0.0025 = 400). An older man with limited exercise capacity and frequent ventricular extrasystoles who continues to smoke following his infarction may have a risk of dying in that next year as high as 10%. A 25% risk reduction for death in such a high-risk patient generates an absolute risk reduction of 2.5% or 0.025, and we would have to treat only 40 such individuals for one year to save a life (1 / 0.025 = 40).
<p> These examples underscore a key element of the decision to start therapy: before deciding on treatment, we must consider our patient's risk of the adverse event if left untreated. For any given RRR, the higher the probability that a patient will experience an adverse outcome if we don't treat, the more likely the patient will benefit from treatment, and the fewer such patients we need to treat to prevent one event. Thus, both patients and our own clinical efficiency benefit when the number we need to treat to prevent an event is low.
<p> We might not hesitate to treat even as many as 400 patients to save one life if the treatment was cheap, easy to apply and comply with, and safe. In reality, however, treatments usually are expensive and they carry risks. When these risks or adverse outcomes are documented in trial reports, users can apply the number needed to treat to judge both the relative benefits and costs of therapy. If, for instance, beta blockers cause clinically important fatigue in 10% of the patients who use them, the NNT to cause fatigue is 1/0.10 or 10. This is shown in Table 4 , where it is seen that a policy of treating low-risk patients after myocardial infarction (NNT = 400 to prevent one death) will result in 40 being fatigued for every life saved. On the other hand, a policy of treating just high-risk patients will result in 4 being fatigued for every life saved.
<h> Table 4: Incorporating side-effects into the Number Needed to be Treated
<p> If the risk of death at one year without therapy (baseline risk) is: X
<p> 1% or 0.01
<p> 10% or 0.10
<p> And the risk of death with propranolol is: Y
<p> 0.01x0.75= 0.0075
<p> 0.10x0.75= 0.075
<p> Then the Absolute risk reduction is: X - Y
<p> 0.01-0.0075=0.0025
<p> 0.10-0.075=0.025
<p> And the Number Needed to be Treated to prevent one event is: 1/(X - Y)
<p> 1/0.0025 = 400
<p> 1/0.025 = 40
<p> And if the incidence of clinically important fatigue on propranolol is:
<p> 10% or 0.10
<p> Then the number of fatigued patients per life saved is:
<p> 400x0.1 = 40
<p> 40x0.1 = 4
<p> Clinicians don't, however, treat groups of patients uniformly. Rather, we consider individual responses and tailor our therapy accordingly. One response to the problem of common, relatively minor side effects (such as fatigue) is to discontinue therapy in patients suffering from that problem. If we think of fatigued low-risk patients as a group, we would make 400 patients fatigued to save a life, a trade-off that probably wouldn't be worth it. By discontinuing treatment in these people, we can treat the remainder without making anyone fatigued.
<p> We cannot apply this approach, however, to severe, episodic events. Examples include the risk of bleeding in patients given anticoagulants, thrombolytic agents, or aspirin, or the risk of rare but devastating drug reactions. In each of these examples the number of adverse events per life saved (or, if the events are rare enough, the number of lives saved per adverse event) can provide a compelling picture of the trade offs associated with the intervention.
<h> Resolution of the Scenario
<p> In the randomized trial of warfarin in non-valvular atrial fibrillation that you selected for reading, 260 patients received warfarin and 265 received placebo. The results are summarized in Table 5.
<h> Table 5: Summary of the effect of warfarin therapy on patients with non-valvular atrial fibrillation
<p> If the risk of stroke at one year without therapy (baseline risk) is: X
<p> Over the next year-and-a-half, just 4 of the former (0.9% per year), but 19 of the latter (4.3% per year) suffered cerebral infarction. Thus, the relative risk reduction (RRR) is (0.043 - 0.009)/0.043 = 79%, the absolute risk reduction is 0.043-0.009 = 0.034, and the number needed to be treated to prevent one stroke is 1/0.034 = 29 (or approximately 30). Applying confidence intervals to this number needed to treat, the number needed to treat could be (using the lower boundary of the confidence interval around the RRR, which was 0.52) as great as 45, or (using the upper boundary of the confidence interval around the RRR, which was 0.90) as few as 17. Now, you know that warfarin is a potentially dangerous drug, and that about 1% of patients on this treatment will suffer clinically important bleeding as a result of treatment each year [31] . Therefore, there will be one bleed in every 100 treated patients, and if the number needed to treat to prevent a stroke is 30, then for every 3 strokes prevented, one major episode of bleeding would occur. If the lower boundary of the confidence interval for the benefit of oral anticoagulants represents the truth, the NNT is 45 and for every two strokes prevented, one would cause a major bleed; if, on the other hand, the upper boundary represents the truth, the NNT is 26 and approximately 4 strokes would be prevented for every major bleeding episode. The true risk-benefit ratio probably lies somewhere between these extremes, closer to that asssociated with the point estimate.
<p> And what about the woman with lupus nephritis, whose plight, described in Part A of this two-part essay, prompted us to find that trial of adding plasmapheresis to a regimen of prednisone and cyclophosphamide? Unfortunately, although plasmapheresis did produce sharp declines in the substituted end-points of anti-dsDNA antibodies and cryoprecipitable immune complexes, the trial did not find any benefit from plasmapheresis in the clinically important measures of renal failure or mortality. When a careful statistical analysis of the emerging data suggested little hope of ever showing clinical benefit, the trial was stopped.
<h> Conclusion
<p> Having read the introduction to this series and the two articles on using articles about therapy, we hope that you are developing a sense of how to use the medical literature to resolve a treatment decision. First, define the problem clearly, and use one of a number of search strategies to obtain the best available evidence. Having found an article relevant to the therapeutic issue, assess the quality of the evidence. To the extent that the quality of the evidence is poor, any subsequent inference (and the clinical decision it generates) will be weakened. If the quality of the evidence is adequate, determine the range within which the true treatment effect likely falls. Then, consider the extent to which the results are generalizable to the patient at hand, and whether the outcomes that have been measured are important. If the generalizability is in doubt, or the importance of the outcomes questionable, support for a treatment recommendation will be weakened. Finally, by taking into account the patient's risk of adverse events, assess the likely results of the intervention. This involves a balance sheet looking at the probability of benefit, and the associated costs (including monetary costs, and issues such as inconvenience) and risks. The bottom line of the balance sheet will guide your treatment decision.
<p> While this may sound like a challenging route to deciding on treatment, it is what clinicians implicitly do each time they administer therapy [32] . Making the process explicit, and being able to apply guidelines to help assess the strength of evidence will, we think, result in better patient care.
<p> 24. Detsky AS, Sackett DL. When was a "negative" clinical trial big enough? How many patients you needed depends on what you found When was a "negative" clinical trial big enough? How many patients you needed depends on what you found. Arch Intern Med 145. 709-12 (1985).

